{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OurOwnFunctions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suaif/Video_descriptor/blob/master/OurOwnFunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rXf263b0Mgy",
        "colab_type": "text"
      },
      "source": [
        "In order to have our code more organized, we decided to create this document `OurOwnFunctions.ipynb.`\n",
        "\n",
        "This way we can implement this document as a module in the main document  `main.ipynb.`\n",
        "\n",
        "---\n",
        "\n",
        "**List of implemented functions:**\n",
        "\n",
        "* `slice_video`\n",
        "* `padding`\n",
        "* `int2str`\n",
        "* `clean_text`\n",
        "* `decode_sequence`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPl4XR9rCBNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Necessary modules to use the functions\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import re    as re\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgJgIhNqwAa5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def slice_video(video_id, pathIn, pathOut, height=240, width=240, n_frames=6, format='.avi'):\n",
        " \n",
        "  '''\n",
        "  Given a video gets 'n_frames' images and save them in 'pathOut'.\n",
        "  Image size will be (height, width)\n",
        " \n",
        "  -\n",
        "  Parameters:\n",
        "    video_id         (string)  [VideoID]\n",
        "    pathIn           (string)  [Videos filepath]\n",
        "    pathOut          (string)  [Save filepath]\n",
        "    height           (int)     [Frame size 1]\n",
        "    width            (int)     [Frame size 2]\n",
        "    n_frames         (int)     [Number of frames to obtain]\n",
        "    format           (string)  [Video format '.*' ]\n",
        "  '''\n",
        "\n",
        "  # -- Parameters\n",
        "  vidcap     = cv2.VideoCapture(pathIn + video_id + format)\n",
        "  max_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "  num_img, count = 1, 1\n",
        "  dim = (width, height)\n",
        "\n",
        "  while max_frames % n_frames != 0:\n",
        "     max_frames -= 1\n",
        "\n",
        "  n_fps = max_frames / n_frames\n",
        "  \n",
        "  while vidcap.isOpened and count <= max_frames:\n",
        "\n",
        "    vidcap.set(cv2.CAP_PROP_FPS, 1)    \n",
        "    success,image = vidcap.read()\n",
        "    \n",
        "    # Resize of the image. \n",
        "    resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "    if count % n_fps == 0:\n",
        "     \n",
        "      # We create the route if it doesn't exist\n",
        "      if not os.path.exists(pathOut):  \n",
        "        os.mkdir(pathOut) \n",
        "     \n",
        "      # Save the resized image in the given route\n",
        "      cv2.imwrite(pathOut + video_id + '_' + str(num_img) + \".jpg\", resized)\n",
        "      print(pathOut + video_id + '_' + str(num_img) + \".jpg\")\n",
        "      num_img += 1\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZRIAM1mv6On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padding(Y, maxlen=0, pad='post'):\n",
        "\n",
        "  '''\n",
        "  Fill in zeros in the remaining spaces of a ndarray.\n",
        "  Also adds an extra 0 at the end\n",
        "  -\n",
        "  Parameters:\n",
        "    Y       (list)    [Descriptions tokenized]\n",
        "    maxlen  (int)     [Max length we want to fill with zeros, extra 0 will be added to this length]\n",
        "    pad     (string)  [Padding type, see: tf.keras.preprocessing.sequence.pad_sequences]\n",
        "  '''\n",
        "\n",
        "  Yt = np.asarray(Y)\n",
        "  Yt = Yt[:,np.newaxis]\n",
        "\n",
        "  if maxlen == 0:\n",
        "    \n",
        "    maxlen = max([len(Yt[i].tolist()[0]) for i in range(len(Yt))]) \n",
        "\n",
        "  Ypad = np.zeros((len(Yt), maxlen+1))\n",
        "\n",
        "  for i in range(len(Yt)):\n",
        "\n",
        "    Ypad[i,:-1] = pad_sequences(Yt[i], maxlen=maxlen,  padding=pad, value=0.0)\n",
        "\n",
        "  return Ypad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYrVx4H4v6lN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def int2str(sec, tokenizer):\n",
        "\n",
        "  '''\n",
        "  Convert a list of integers to a list of words.\n",
        "\n",
        "  -\n",
        "  ParÃ¡metros:\n",
        "    sec        (list)        [List which will be converted to words]\n",
        "    tokenizer  (dictionary)  [Contains the dictionary with the words assignations]\n",
        "  '''\n",
        "\n",
        "  # If the input secuence is a list, we convert it to an array\n",
        "\n",
        "  if type(sec) is list:\n",
        "    sec = np.asarray(sec)\n",
        "\n",
        "  # If there is only one sequence we add another dimension\n",
        "\n",
        "  if len(sec.shape) == 1:\n",
        "    sec = sec[np.newaxis, :]\n",
        "\n",
        "  nsecs, nints = sec.shape\n",
        "\n",
        "  sentences = np.zeros((nsecs), dtype=object) \n",
        "\n",
        "  for i in range(nsecs):\n",
        "    sentences[i] = sec[i].astype(int).tolist()\n",
        "\n",
        "  return tokenizer.sequences_to_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB_bY1SRv7OL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "\n",
        "  '''\n",
        "  Clean the text removing unnecessary characters.\n",
        "  \n",
        "  -\n",
        "  Parameters:\n",
        "    text       (string) [Text to clean]\n",
        "  '''\n",
        "\n",
        "  text = text.lower()\n",
        "\n",
        "  text = re.sub(r\"i'm\", \"i am\", text)\n",
        "  text = re.sub(r\"he's\", \"he is\", text)\n",
        "  text = re.sub(r\"she's\", \"she is\", text)\n",
        "  text = re.sub(r\"it's\", \"it is\", text)\n",
        "  text = re.sub(r\"that's\", \"that is\", text)\n",
        "  text = re.sub(r\"what's\", \"that is\", text)\n",
        "  text = re.sub(r\"where's\", \"where is\", text)\n",
        "  text = re.sub(r\"how's\", \"how is\", text)\n",
        "  text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "  text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "  text = re.sub(r\"\\'re\", \" are\", text)\n",
        "  text = re.sub(r\"\\'d\", \" would\", text)\n",
        "  text = re.sub(r\"\\'re\", \" are\", text)\n",
        "  text = re.sub(r\"won't\", \"will not\", text)\n",
        "  text = re.sub(r\"can't\", \"cannot\", text)\n",
        "  text = re.sub(r\"n't\", \" not\", text)\n",
        "  text = re.sub(r\"n'\", \"ng\", text)\n",
        "  text = re.sub(r\"'bout\", \"about\", text)\n",
        "  text = re.sub(r\"'til\", \"until\", text)\n",
        "  text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "      \n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeYXKvXBuzfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq, encoder_model, decoder_model, max_sentence_len):\n",
        "\n",
        "  '''\n",
        "  Uses the encoder and the decoer models to get the full description secuence\n",
        "  \n",
        "  -\n",
        "  Parameters:\n",
        "    input_seq         (array) [Frames sequence]\n",
        "    encoder_model     (model) [Model to process frames and get states vector]\n",
        "    decoder_model     (model) [Model to predict next word of the sequence]\n",
        "    max_sentence_len  (int)   [Maximum length of the decoded sequence]\n",
        "  '''\n",
        "\n",
        "  \n",
        "  # The encoder model process the frames\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # First character of the target sequence is the start character\n",
        "  # 2 is the token that corresponds to the BOS character\n",
        "\n",
        "  target_seq = [[2]]\n",
        "  target_seq = np.asarray(target_seq)\n",
        "\n",
        "  # To simplify, here we assume a batch of size 1.\n",
        "  stop_condition = False\n",
        "  decoded_sentence = []\n",
        "\n",
        "  while not stop_condition:\n",
        "\n",
        "    # Now, the decoder model predict the next word\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # Sample a token\n",
        "    sampled_token_index = np.argmax(output_tokens)\n",
        "    decoded_sentence.append(sampled_token_index) #sampled_char\n",
        "\n",
        "    # Exit condition: either hit max length or find stop character.\n",
        "    # 3 is the token that corresponds to the EOS character\n",
        "\n",
        "    if (len(decoded_sentence) > max_sentence_len) or (sampled_token_index==3):\n",
        "        stop_condition = True\n",
        "\n",
        "    # Update the target sequence (of length 1).    \n",
        "    target_seq = [[sampled_token_index]]\n",
        "    target_seq = np.asarray(target_seq)\n",
        "\n",
        "    # Update states\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}